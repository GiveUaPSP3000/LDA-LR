{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Get the original data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "                                             context  index\n",
      "0  \"The Hunger Games\" is a Well-Constructed \"Chim...      0\n",
      "1  GREAT!!!!!this game is the best game I have ev...      1\n",
      "2  Satisfied CustomersIt's comfortable, it's ligh...      2\n",
      "3  The Greatest!!!!THIS GAME IS REALLY GREAT YOU ...      3\n",
      "4  Love the movie!Great Adam Sandler movie, a cla...      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We obtained these three types through analysis at the beginning, and we will help validate the model later\n",
    "key_name = ['movi', 'game', 'phone']\n",
    "# read the raw data\n",
    "data = pd.read_csv('data/reviews.csv')\n",
    "# preprocess\n",
    "data['context'] = data.apply(lambda x : str(x['review_title']) + str(x['review_body']),axis=1)\n",
    "for import_name in key_name:\n",
    "    data[f'is_{import_name}'] = data.apply(lambda x : import_name in str(x['review_title']) and import_name in str(x['review_body']),axis=1)\n",
    "data_text = data[['context']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the text, including restoring part of speech and removing stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tengyue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    [hunger, game, construct, chimera, movi, larg,...\n1    [great, game, best, game, play, kindl, awesom,...\n2    [satisfi, customersit, comfort, light, machin,...\n3    [greatest, game, great, buy, play, anakin, obi...\n4    [love, movi, great, adam, sandler, movi, class...\n5    [strong, film, polit, versus, justiceveri, int...\n6    [upset, stomachi, understand, product, upset, ...\n7    [star, laff, kid, think, movi, suck, ive, see,...\n8    [great, littl, kidsmi, granddaught, love, play...\n9    [question, answer, great, continu, samara, loo...\nName: context, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "# Preprocessing the text, including restoring part of speech and removing stop words\n",
    "processed_docs = documents['context'].map(preprocess)\n",
    "processed_docs[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abernathi\n",
      "1 acquaint\n",
      "2 add\n",
      "3 administr\n",
      "4 adventur\n",
      "5 afraid\n",
      "6 agreeabl\n",
      "7 alic\n",
      "8 aliv\n",
      "9 alli\n",
      "10 amanida\n"
     ]
    }
   ],
   "source": [
    "# Generating word matrix\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "# Design dictionary threshold\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.7, keep_n=100000)\n",
    "# Generate a corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# Embedding all words\n",
    "data['bow_vector'] = data['context'].apply(lambda x : dictionary.doc2bow(preprocess(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Built-in function, input text to get topic predictions\n",
    "def test_sign(text):\n",
    "    score_hi = 0\n",
    "    topic_hi = -1\n",
    "    for index, score in sorted(lda_model[text], key=lambda tup: -1*tup[1]):\n",
    "        if score > score_hi:\n",
    "            topic_hi = index\n",
    "            score_hi = score\n",
    "    return topic_hi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# The optimal number of classification was obtained by model evaluation\n",
    "score_h = 0\n",
    "best_type = 2\n",
    "for i in range(3,20):\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=i, id2word=dictionary, passes=2, workers=2)\n",
    "    data['topic'] = data['bow_vector'].apply(lambda x : test_sign(x))\n",
    "    game_topic = data[data['is_game'] == True]['topic'].value_counts()\n",
    "    movie_topic = data[data['is_movi'] == True]['topic'].value_counts()\n",
    "    phone_topic = data[data['is_phone'] == True]['topic'].value_counts()\n",
    "    main_game_topic = game_topic.idxmax()\n",
    "    main_movie_topic = movie_topic.idxmax()\n",
    "    main_phone_topic = phone_topic.idxmax()\n",
    "    if main_game_topic != main_movie_topic and main_movie_topic != main_phone_topic:\n",
    "        score_tp = (game_topic.max() / game_topic.sum()) + (main_movie_topic.max() / main_movie_topic.sum()) + (main_phone_topic.max() / main_phone_topic.sum())\n",
    "        if score_tp > score_h:\n",
    "            score_h = score_tp\n",
    "            best_type = i\n",
    "\n",
    "# show the best performance and the number of the types\n",
    "print(score_h)\n",
    "print(best_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# train the model with the best types number\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=best_type, id2word=dictionary, passes=2, workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## predict and show the topic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.051*\"game\" + 0.028*\"movi\" + 0.021*\"like\" + 0.020*\"play\" + 0.019*\"great\" + 0.017*\"love\" + 0.017*\"fun\" + 0.016*\"good\" + 0.013*\"time\" + 0.009*\"app\"\n",
      "Topic: 1 \n",
      "Words: 0.009*\"film\" + 0.006*\"like\" + 0.005*\"match\" + 0.005*\"movi\" + 0.005*\"song\" + 0.005*\"album\" + 0.005*\"time\" + 0.005*\"good\" + 0.005*\"best\" + 0.004*\"man\"\n",
      "Topic: 2 \n",
      "Words: 0.028*\"quot\" + 0.010*\"game\" + 0.008*\"like\" + 0.006*\"new\" + 0.006*\"time\" + 0.005*\"play\" + 0.004*\"charact\" + 0.004*\"world\" + 0.004*\"way\" + 0.004*\"level\"\n",
      "Topic: 3 \n",
      "Words: 0.015*\"work\" + 0.014*\"use\" + 0.011*\"great\" + 0.009*\"like\" + 0.009*\"good\" + 0.008*\"phone\" + 0.008*\"product\" + 0.007*\"case\" + 0.007*\"buy\" + 0.007*\"need\"\n",
      "Topic: 4 \n",
      "Words: 0.015*\"like\" + 0.013*\"season\" + 0.012*\"good\" + 0.012*\"great\" + 0.011*\"love\" + 0.009*\"product\" + 0.008*\"episod\" + 0.008*\"dog\" + 0.007*\"dvd\" + 0.007*\"buy\"\n",
      "Topic: 5 \n",
      "Words: 0.025*\"film\" + 0.021*\"movi\" + 0.008*\"stori\" + 0.007*\"watch\" + 0.007*\"charact\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"great\" + 0.006*\"love\" + 0.005*\"good\"\n"
     ]
    }
   ],
   "source": [
    "# predict the topic\n",
    "data['topic'] = data['bow_vector'].apply(lambda x : test_sign(x))\n",
    "# show each topic's key words\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## merge the same product_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0    15891\n5    12256\n3     9093\n4     7859\n1     3415\n2     1486\nName: topic, dtype: int64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the original prediction\n",
    "data['topic'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29660 [00:00<?, ?it/s]D:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "100%|█████████▉| 29643/29660 [02:32<00:00, 189.99it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pro_id = data['product_id'].unique()\n",
    "bar = tqdm(total=len(pro_id))\n",
    "for i in pro_id:\n",
    "    temp_df = data[data['product_id'] == i]\n",
    "    r_ture = temp_df['topic'].value_counts().idxmax()\n",
    "    data[data['product_id'] == i]['topic'] = r_ture\n",
    "    bar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0    15891\n5    12256\n3     9093\n4     7859\n1     3415\n2     1486\nName: topic, dtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the prediction after merge\n",
    "data['topic'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# sort and save\n",
    "data = data.sort_values('review_id')\n",
    "save_df = data[['review_id', 'topic']]\n",
    "save_df.columns = ['review_id', 'product_category']\n",
    "save_df.to_csv('task1a.csv', index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}